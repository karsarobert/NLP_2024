{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPB36208dUSnSMGQ0qqrKL/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karsarobert/NLP_2024/blob/main/05/NLP2024_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k"
      ],
      "metadata": {
        "id": "Uy4CZkTkLrVq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd87d39-1071-41da-fc2c-94858fc81f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.12.3)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (9.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (6.0.1)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (4.9.4)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (3.8.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.31.0)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.1.1-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.7/97.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from newspaper3k) (2.8.2)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.16.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.1->newspaper3k) (4.66.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->newspaper3k) (2024.2.2)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-2.0.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.13.1)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13538 sha256=669b421c3deb5d36faa11949941c4f9753bb94879676e38f24102d59ed12f9d3\n",
            "  Stored in directory: /root/.cache/pip/wheels/c8/d6/6c/384f58df48c00b9a31d638005143b5b3ac62c3d25fb1447f23\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3340 sha256=8847609c527344b685b0f11a01c2f5ae320d29aa41f0a18daa429f1ccebef9f3\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/02/e7/a1ff1760e12bdbaab0ac824fae5c1bc933e41c4ccd6a8f8edb\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398381 sha256=8a7e10de35edf78d42fd3edd019670ab7dd45e0fff8f3bac21a5dca5611114a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/7a/c4/0c/12a9a314ecac499456c4c3b2fcc2f635a3b45a39dfbd240299\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6048 sha256=ee32c6982b9fcd8bc46da670eb1c1ba9e8b15d1924afd0bee3373c10085c2aef\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.2.0 feedfinder2-0.0.4 feedparser-6.0.11 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-2.0.0 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.origo.hu/hirarchivum/2024/01/02"
      ],
      "metadata": {
        "id": "BfztUuz_yq20"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importáljuk a datetime modult, amely segít kezelni a dátumokat\n",
        "import datetime\n",
        "\n",
        "# Létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "\n",
        "# Létrehozunk egy datetime objektumot 2023. január 1-jével\n",
        "date = datetime.date(2024, 1, 1)\n",
        "\n",
        "# Létrehozunk egy másik datetime objektumot 2024. január 1-jével\n",
        "end_date = datetime.date(2024, 1, 31)\n",
        "\n",
        "# Végigmegyünk a dátumokon egy napos lépésekkel, amíg el nem érjük a végdátumot\n",
        "while date < end_date:\n",
        "    # Kinyerjük a dátum évét, hónapját és napját\n",
        "    year = date.year\n",
        "    month = date.month\n",
        "    day = date.day\n",
        "    # Összerakjuk az url-t a dátummal\n",
        "    url = f\"https://www.origo.hu/hirarchivum/{year}/{month}/{day}\"\n",
        "    # Hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "    # Növeljük a dátumot egy nappal\n",
        "    date += datetime.timedelta(days=1)\n",
        "\n",
        "# Kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVZehGgcLfVi",
        "outputId": "8cc81a0a-294b-4e1e-f698-669b74862585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['https://www.origo.hu/hirarchivum/2024/1/1', 'https://www.origo.hu/hirarchivum/2024/1/2', 'https://www.origo.hu/hirarchivum/2024/1/3', 'https://www.origo.hu/hirarchivum/2024/1/4', 'https://www.origo.hu/hirarchivum/2024/1/5', 'https://www.origo.hu/hirarchivum/2024/1/6', 'https://www.origo.hu/hirarchivum/2024/1/7', 'https://www.origo.hu/hirarchivum/2024/1/8', 'https://www.origo.hu/hirarchivum/2024/1/9', 'https://www.origo.hu/hirarchivum/2024/1/10', 'https://www.origo.hu/hirarchivum/2024/1/11', 'https://www.origo.hu/hirarchivum/2024/1/12', 'https://www.origo.hu/hirarchivum/2024/1/13', 'https://www.origo.hu/hirarchivum/2024/1/14', 'https://www.origo.hu/hirarchivum/2024/1/15', 'https://www.origo.hu/hirarchivum/2024/1/16', 'https://www.origo.hu/hirarchivum/2024/1/17', 'https://www.origo.hu/hirarchivum/2024/1/18', 'https://www.origo.hu/hirarchivum/2024/1/19', 'https://www.origo.hu/hirarchivum/2024/1/20', 'https://www.origo.hu/hirarchivum/2024/1/21', 'https://www.origo.hu/hirarchivum/2024/1/22', 'https://www.origo.hu/hirarchivum/2024/1/23', 'https://www.origo.hu/hirarchivum/2024/1/24', 'https://www.origo.hu/hirarchivum/2024/1/25', 'https://www.origo.hu/hirarchivum/2024/1/26', 'https://www.origo.hu/hirarchivum/2024/1/27', 'https://www.origo.hu/hirarchivum/2024/1/28', 'https://www.origo.hu/hirarchivum/2024/1/29', 'https://www.origo.hu/hirarchivum/2024/1/30']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from newspaper import Article\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Egy lista a magyar politikai weboldalakról, amelyekről híreket akarsz kaparni\n",
        "\n",
        "# Egy üres lista a hírek tárolására\n",
        "news = []\n",
        "neutral_news = []\n",
        "itthon_links = []\n",
        "\n",
        "# Végigmegyünk a weboldalak listáján\n",
        "for website in urls:\n",
        "    # Lekérjük a weboldal tartalmát\n",
        "    response = requests.get(website)\n",
        "    # Létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Megkeressük az összes hírcímet tartalmazó HTML elemet (ez változhat az oldaltól függően)\n",
        "    links = soup.find_all(\"a\", href=True)\n",
        "    # Végigmegyünk a hírcímeken\n",
        "    for link in links:\n",
        "      if link.find(class_=\"categorized-article-container-title\") is not None:\n",
        "        title = link.find(class_=\"categorized-article-container-title\").text\n",
        "    # Kinyerjük a hírcím szövegét\n",
        "      href = link[\"href\"]\n",
        "      if href.startswith(\"/itthon/\"):\n",
        "          # Létrehozunk egy Article objektumot a linkkel\n",
        "          article = Article(f'https://origo.hu'+href)\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "          try:\n",
        "            article.download()\n",
        "            article.parse()\n",
        "\n",
        "          # Kinyerjük a cikk szövegét\n",
        "            text = article.text\n",
        "          # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "            news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "          except:\n",
        "            print(\"Hiba történt egy cikk letöltése vagy elemzése közben\")\n",
        "      elif href.startswith('/auto/') or href.startswith('/tudomany/') or href.startswith('/sport/'):\n",
        "          # Létrehozunk egy Article objektumot a linkkel\n",
        "          article = Article(f'https://origo.hu'+href)\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "          article.download()\n",
        "          article.parse()\n",
        "          # Kinyerjük a cikk szövegét\n",
        "          text = article.text\n",
        "          # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "          neutral_news.append({\"title\": title, \"link\": f'https://origo.hu'+href, \"text\": text})\n",
        "\n",
        "# Létrehozunk egy pandas dataframe-et a hírek listájából\n",
        "df = pd.DataFrame(news)\n",
        "\n",
        "# Elmentjük a dataframe-et egy CSV fájlba\n",
        "df.to_csv(\"origo_news.csv\", index=False)"
      ],
      "metadata": {
        "id": "cM5RaIsTM13v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec65eae-328c-4819-b5df-df8b7c9410dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hiba történt a cikk letöltése vagy elemzése közben:\n",
            "Hiba történt a cikk letöltése vagy elemzése közben:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "article = Article('https://www.origo.hu/itthon/2024/01/jarnak-vonat-nagymaros-tehervonat-veroce')\n",
        "          # Letöltjük és elemezzük a cikk HTML-jét\n",
        "article.download()\n",
        "article.parse()\n",
        "          # Kinyerjük a cikk szövegét\n",
        "text = article.text\n",
        "\n"
      ],
      "metadata": {
        "id": "5mD85kz90XGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    response = requests.get(urls[0])\n",
        "    # Létrehozunk egy BeautifulSoup objektumot a HTML elemzéséhez\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "    # Megkeressük az összes hírcímet tartalmazó HTML elemet (ez változhat az oldaltól függően)\n",
        "    links = soup.find_all(\"a\", href=True)"
      ],
      "metadata": {
        "id": "lHE4P3Cu0q-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://telex.hu/archivum?oldal=1"
      ],
      "metadata": {
        "id": "lcMCAIRDBhgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Létrehozunk egy üres listát az url-ek tárolására\n",
        "urls = []\n",
        "start = 1\n",
        "end = 10 # jelenleg 7539 a legnagyobb lehetséges érték\n",
        "\n",
        "# Végigmegyünk a dátumokon egy napos lépésekkel, amíg el nem érjük a végdátumot\n",
        "while start < end:\n",
        "    # Formázzuk a dátumot úgy, hogy két számjegyű legyen a hónap és a nap\n",
        "    date_str = f\"{start}\"\n",
        "    # Összerakjuk az url-t a dátummal\n",
        "    url = f\"https://telex.hu/archivum?oldal={date_str}\"\n",
        "    # Hozzáadjuk az url-t a listához\n",
        "    urls.append(url)\n",
        "    # Növeljük a dátumot egy nappal\n",
        "    start += 1\n",
        "\n",
        "# Kiírjuk az url-ek listáját\n",
        "print(urls)"
      ],
      "metadata": {
        "id": "aJIVO9gsCzBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "url = 'https://telex.hu/archivum?oldal=1'\n",
        "news = []\n",
        "\n",
        "for website in urls:\n",
        "  try:\n",
        "      response = requests.get(website)\n",
        "      response.raise_for_status()\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      print(e)\n",
        "      sys.exit(1)\n",
        "\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  articles = soup.find_all('a', class_=\"list__item__title\", href=True)\n",
        "\n",
        "  df = pd.DataFrame(columns=['Title', 'Link', 'Text'])\n",
        "\n",
        "  for article in articles:\n",
        "      if article['href'].find('/belfold/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # Letöltjük és elemezzük a cikk HTML-jét\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        # Kinyerjük a cikk szövegét\n",
        "        text = article.text\n",
        "        # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "      elif article['href'].find('/kult/') == 0 or article['href'].find('/tudomany/') == 0 or article['href'].find('/sport/') == 0:\n",
        "        title = article.find(class_=\"hasHighlight\").text.strip()\n",
        "        link = f'https://telex.hu' + article[\"href\"]\n",
        "        #df = df.append({'title': title, 'link': link}, ignore_index=True)\n",
        "        article = Article(link)\n",
        "        # Letöltjük és elemezzük a cikk HTML-jét\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        # Kinyerjük a cikk szövegét\n",
        "        text = article.text\n",
        "        # Hozzáadjuk a hírt a listához egy szótár formájában\n",
        "        neutral_news.append({\"title\": title, \"link\": link, \"text\": text})\n",
        "\n",
        "df = pd.DataFrame(news)\n",
        "dfN = pd.DataFrame(neutral_news)\n",
        "df.to_csv(\"telex_news.csv\", index=False)\n",
        "dfN.to_csv(\"neutral_news.csv\", index=False)"
      ],
      "metadata": {
        "id": "r-rwFmQKsCux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "origo = pd.read_csv('origo_news.csv')\n",
        "origo['target']=origo['target']=0\n",
        "telex = pd.read_csv('telex_news.csv')\n",
        "telex['target'] = telex['target']=1\n",
        "neutral = pd.read_csv('neutral_news.csv')\n",
        "neutral['target'] = neutral['target']=2"
      ],
      "metadata": {
        "id": "1Aa12PyCCO4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = pd.concat([origo, telex, neutral])\n"
      ],
      "metadata": {
        "id": "NqNo5BlE6AwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = allnews[['target','text']]"
      ],
      "metadata": {
        "id": "WWqaDrfWCvTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews = allnews.dropna()"
      ],
      "metadata": {
        "id": "nV8xQX29A1jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "allnews.to_csv(\"allnews.csv\", index=False)\n",
        "files.download('allnews.csv')"
      ],
      "metadata": {
        "id": "xtzN9dY6MoAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#A modell tanítása"
      ],
      "metadata": {
        "id": "Z93vzLoX0Ue7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1pLpK71QeCmXx3yzL6A8oRx2jl1q0J-XJ"
      ],
      "metadata": {
        "id": "7DBIfn0eXLqu",
        "outputId": "ad652cac-001b-4a7a-e92e-04419be0db9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1pLpK71QeCmXx3yzL6A8oRx2jl1q0J-XJ\n",
            "To: /content/allnews.csv\n",
            "100% 48.2M/48.2M [00:00<00:00, 82.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "allnews = pd.read_csv('allnews.csv')"
      ],
      "metadata": {
        "id": "J9tkUV3M01uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "allnews.shape"
      ],
      "metadata": {
        "id": "qGV8DmhT60Ps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = allnews['text']\n",
        "y = allnews['target']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "wZy6-kny6UAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(1,3))\n",
        "\n",
        "x_train_tfidf = vec.fit_transform(X_train)\n",
        "x_test_tfidf = vec.transform(X_test)"
      ],
      "metadata": {
        "id": "zUFE32dRTh9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf_tfidf = MultinomialNB(fit_prior=True)\n",
        "clf_tfidf.fit(x_train_tfidf, y_train)\n",
        "y_test_pred_tfidf = clf_tfidf.predict(x_test_tfidf)"
      ],
      "metadata": {
        "id": "s7quiIMaX5AE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_test_pred_tfidf) #\n",
        "print(report)"
      ],
      "metadata": {
        "id": "CHV75hgvYg5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vec = CountVectorizer(analyzer='char', ngram_range=(1,2)) # analyzer='char', ngram_range=(1,5) próbáljuk ki a karakterszintű tokenizálással is\n",
        "\n",
        "x_train_cv = vec.fit_transform(X_train)\n",
        "x_test_cv = vec.transform(X_test)"
      ],
      "metadata": {
        "id": "NSrtRBZ6MTff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB(fit_prior=True) # Az osztály előzetes valószínűségeinek megtanulása, ha hamis akkor egyenletes előfeltevést használunk.\n",
        "clf.fit(x_train_cv, y_train)\n",
        "y_test_pred = clf.predict(x_test_cv)"
      ],
      "metadata": {
        "id": "-kPOa0XuMiRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_test, y_test_pred) #\n",
        "print(report)"
      ],
      "metadata": {
        "id": "SZs6I7EgMm2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test.iloc[1])"
      ],
      "metadata": {
        "id": "kzlRXuXoBpoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = vec.transform(pd.Series(['A kormány május 30-án nyújtja be a jövő évi költségvetés tervezetét az Országgyűlésnek, amely július 7-én fogadhatja el azt - mondta Gulyás Gergely Miniszterelnökséget vezető miniszter csütörtöki sajtótájékoztatóján, Budapesten. A tárcavezető kifejtette: számtalan nehézség közepette kell megtervezni a jövő évi büdzsét, \"hiszen háborús időket élünk\". Olyan költségvetésre van szükség, amely garantálja az ország biztonságát, megvédi a családokat, a nyugdíjakat, a munkahelyeket és a rezsicsökkentést - mondta.']))"
      ],
      "metadata": {
        "id": "32vGEY5IFkWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(a):\n",
        "  if a==0:\n",
        "    print('Origo')\n",
        "  elif a==1:\n",
        "    print('Telex')\n",
        "  elif a==2:\n",
        "    print('Semleges')"
      ],
      "metadata": {
        "id": "J6HFfuwNGKeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = clf.predict(a)\n",
        "decode(pred)"
      ],
      "metadata": {
        "id": "gaWMzK6cBuvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = {0: \"Origo\", 1: \"Telex\", 2: \"Semleges\"}"
      ],
      "metadata": {
        "id": "YSdv2gLeE1_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label[pred[0]]"
      ],
      "metadata": {
        "id": "PlBSqIkHE4-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZPSI6d4E8Pj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}